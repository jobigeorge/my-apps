---
layout:     post
title:      "PCs and web2.0 : Part 2 PCs the perfect Interaction Engines"
subtitle:   ""
description: ""
excerpt: ""
date:       2006-11-05 11:37:00
author:     " Jobi George"
image:     "static/img/grey-transparent.jpg"
published: true
showtoc: false 
tags:
    - Strategy
    - Ecosystem
    - Business
URL: "/2006/11/05/personal-computer-part2/"
categories: [ Business ]
---
[Last week](/2006/10/19/personal-computers-part1) I talked about how the evolving push towards services-based applications threatens the existing PCs role. There is a widespread fear that web2.0 and SaaS will finally kill the thick Moore's law-driven PC, and we all will be either working on dumb terminals or the cell phone size devices that will run all our applications.  !["Move towards Thick Compute Thin State"](/img/blogimg/20061105-thickthin.jpg)


Skeptics and realists are arriving and last week's blog by[John Milan](http://intelligantt.blogspot.com/) talked about how [Google's desktop based application strategy](http://www.readwriteweb.com/archives/elephants_and_evolution.php) is evolving. I believe that PC type high-end computing devices will still be around but certainly with a newly defined role of providing stateless and cheap raw computes.

PC will take on more of the local storage/caching/execution device going forward. It is already happening, PC in the home is already the synching & charging station, music mixer & browsing device, [Sam Ruby](http://intertwingly.net/slides/2006/npuc/) from IBM has an excellent presentation on the topic. While in the enterprise, PCs are quickly becoming thin state compute players, with role-based applications/kiosks becoming commonplace.


**Thick Compute Thin State**

The end-user client nodes, though losing a lot of application-level computes to the cloud, is certainly gaining a lot of interaction level computes. Be it the browser based application that uses a lot of rich  [AJAX](http://www.adaptivepath.com/publications/essays/archives/000385.php) code such as Zimbra or the role based deployments of enterprise applications. I remember somebody mentioning me that the [Zimbra](http://www.zimbra.com/)demos looked awful until Intel Core2Duo showed up, especially in the case of Apple Macs. With loosely coupled applications and mashups happening at the last mile, the interaction level computes are bound to arise at the point of interactions. Additionally, increasing desire for flexibility and agility prompts development at higher level abstracted languages, pushing performance to a back burner and consuming MIPS very inefficiently.

The tipping of broadband adoption beyond 50%, the availability of cheap hardware, and open-source stacks have finally brought in the ability to break out from the limitations of the client-server model. Applications will not be written in the old ways, which means applications can finally be experienced differently. Application streaming vendors and role-based deployment stacks are bridging the gap for existing client-server applications in the enterprise space, while in the consumer space, everybody seems to be rallying behind web2.0


**PC going from Multi Applications to Multi-Player**

 It was while working on the [PDS](http://www.usenix.org/publications/library/proceedings/vee05/full_papers/p175-alpern.pdf) project at [IBM T.J Watson lab](http://www.watson.ibm.com/index.shtml) that we coined the term "Application Player" for the first time. Applications can be experienced as a stream similar to watching a MPEG file or internet radio stream. The real implication was that now one could treat compiled applications and play them like any media using an S/W player. !["Move towards Thick Compute Thin State"](/img/blogimg/20061105-pcclient.jpg)
 
 This meant that the composition and packaging of the application were independent of the way applications are run. After all, we always knew that the runtime characteristics of an application are different from the design time and compose time characteristics; we didn't know how to manage it differently. By separating these attributes, it is now possible to pull in some of the composition and assembly aspects of the application into the cloud. At the same time, it also makes the edge a better runtime stateless player. I think Microsoft is also thinking in the same line, and the increased focus on declarative languages like XUL, XAML, FLEX makes it easier to reach there.

This is the most significant opportunity for PC. PCs can now become the perfect form of **interaction engine**. Finally, the glue that stuck OS and applications to PCs is loosening, and PCs can be redefined into a platform for interaction. Things like device driver models that have become a nightmare inside the Operating system could be pulled back to the hardware and used to putting up a softer face. It is time for us to look beyond the keyboard and mouse interfaces and provide an interaction-based programming interface. Tools that can be applied to multi-touch, voice, conversational systems, 360Â° camera. PC's or PC type devices will start becoming the enabler of the [local infoclound](http://vanderwal.typepad.com/personal_infocloud/). And with the onset of virtualization technology in chips and open-source VMMs, now we have the basic building blocks to build this.

I am excited by the opportunity and believe this is just the beginning. The whole service orientation of applications and the ability to experience the Application Anywhere Anytime will finally bring the information to fingertips. It is heralding in a new era for computer science.

PC is marching towards becoming the perfect "Interaction Engine," and who knows how many unintended uses will emerge. Here is one that is using existing PC hardware to predict [Tsunami's using vibrations on your hard disk](http://www.ninsight.at/tsunami/)

_______________
Originally posted at 
[ Blogspot ](http://jobig.blogspot.com/2006/11/pcs-and-web20-part-2-pcs-perfect.html) 
on **{{< param date >}}** 